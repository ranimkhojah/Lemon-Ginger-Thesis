{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.models as g\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from scipy import spatial\n",
    "import os\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from gensim.utils import simple_preprocess\n",
    "import re\n",
    "import codecs\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.spatial.distance import euclidean, pdist, squareform\n",
    "from scipy import sparse\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec Model\n",
    "model=\"ms/doc2vec.bin\"\n",
    "test_docs=\"data/test_docs.txt\"\n",
    "output_file=\"data/test_vectors.txt\"\n",
    "#parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "#load model as 'd2v'\n",
    "d2v = g.Doc2Vec.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test cases from a directory\n",
    "folder = \"folder\"\n",
    "tcs = []\n",
    "dis_mat = []\n",
    "def all_file_content(directory_name):\n",
    "    file_list = os.listdir(directory_name)\n",
    "    for file_name in file_list:\n",
    "        with open(os.path.join(directory_name, file_name), \"r\") as src_file:\n",
    "            yield src_file.read().lower()\n",
    "for file_content in all_file_content(folder):\n",
    "    file_list = os.listdir(folder)\n",
    "    tcs.append(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and lemmatize files content\n",
    "def tokenize(posts):\n",
    "    tokenized_posts = []\n",
    "    for row in posts:\n",
    "        token = word_tokenize(row)\n",
    "        if token != '':\n",
    "            tokenized_posts.append(token)\n",
    "    return tokenized_posts\n",
    "\n",
    "def lemmatize(tokenized_posts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_posts = []\n",
    "    for sentence in tokenized_posts:\n",
    "        tagged = pos_tag(sentence)\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in tagged:\n",
    "            wntag = get_wordnet_pos(tag)\n",
    "            if wntag is None:\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, pos=wntag))\n",
    "        lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "        lemmatized_posts.append(lemmatized_sentence)\n",
    "    return lemmatized_posts\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    porter =  PorterStemmer()\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "# print(stemSentence(tcs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the input\n",
    "\n",
    "#remove stop words\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word.lower() not in STOP_WORDS])\n",
    "\n",
    "#remove URLs\n",
    "def cleanURL(data):\n",
    "    regex=re.compile (r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return regex.sub(' ', data)\n",
    "#remove puncts\n",
    "def remove_punc(post):\n",
    "    new_post = ''\n",
    "    for ch in post:\n",
    "        new_post += checkForCH(ch)\n",
    "    return new_post\n",
    "\n",
    "def checkForCH(ch):\n",
    "    CharToSpace = ',-'\n",
    "    if ch.isalpha():\n",
    "        return ch\n",
    "    elif ch in (CharToSpace) or ch.isspace():\n",
    "        return ' '\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# remove non-latin characters\n",
    "def filterLatinAlphabet(data):\n",
    "    regex = re.compile(r'[^\\x00-\\x7F\\x80-\\xFF\\u0100-\\u017F\\u0180-\\u024F\\u1E00-\\u1EFF]')\n",
    "    return regex.sub('', data)\n",
    "\n",
    "#remove non-english words\n",
    "def load_words():\n",
    "    with open('words_alpha.txt') as word_file:\n",
    "        valid_words = set(word_file.read().split())\n",
    "\n",
    "    return valid_words\n",
    "\n",
    "def clean_nonenglish_words(sentence, vocabulary=load_words()):\n",
    "    for word in sentence:\n",
    "        if word.casefold() not in vocabulary or word == \"w\":\n",
    "            sentence.remove(word)\n",
    "\n",
    "def clean_nonenglish_words_printing(sentence, vocabulary=load_words()):\n",
    "    for word in sentence:\n",
    "        if word.casefold() not in vocabulary:\n",
    "            sentence.remove(word)\n",
    "            print(word, \"not english\")\n",
    "        if word.casefold() in vocabulary:\n",
    "            print(word, \"ENGLISH\")\n",
    "            \n",
    "def clean_sentence(sentence):\n",
    "    sentence = filterLatinAlphabet(sentence)\n",
    "    sentence = cleanURL(sentence)\n",
    "    sentence = remove_punc(sentence)\n",
    "    sentence = sentence.split()\n",
    "    clean_nonenglish_words(sentence)\n",
    "    return sentence\n",
    "\n",
    "lemmatized_text = stemSentence(tcs[1])\n",
    "# print(clean_sentence(lemmatized_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetorizing\n",
    "vectors = [d2v.infer_vector(clean_sentence(stemSentence(tc)), alpha=start_alpha, steps=infer_epoch) for tc in tcs]\n",
    "vectors_sparse = sparse.csr_matrix(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating pairwise distances and create the similarity matrix\n",
    "distances = cosine_distances(vectors_sparse)\n",
    "# print('pairwise dense output:\\n {}\\n'.format(similarities))\n",
    "distances_sparse = cosine_similarity(vectors_sparse,dense_output=False)\n",
    "# print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\n",
    "\n",
    "#add headers to the distance matrix\n",
    "df = pd.DataFrame(distances, index=file_list, columns=file_list)\n",
    "\n",
    "#export distance matrix as .csv file\n",
    "df.to_csv('d2v_dm.csv', index=True, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_py",
   "language": "python",
   "name": "thesis_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
